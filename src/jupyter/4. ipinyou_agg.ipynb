{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating IPinYou Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll be creating a Spark Structured Streaming aggregation. We'll read Avro objects from our <b>ipinyou</b> Kafka topic, and write the aggregated data to the <b>ipinyou-agg</b> Kafka topic as Avro once again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we set up our streaming aggregation, let's backfill our ipinyou-agg topic with some pre-aggregated data in order to simulate historical events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ShowTypes on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.avro.Schema\n",
    "import org.apache.avro.generic.GenericData\n",
    "import org.apache.avro.generic.GenericRecord\n",
    "import org.apache.kafka.clients.producer.KafkaProducer\n",
    "import org.apache.kafka.clients.producer.ProducerConfig\n",
    "import org.apache.kafka.clients.producer.ProducerRecord\n",
    "import io.confluent.kafka.serializers.{KafkaAvroDecoder, KafkaAvroSerializer}\n",
    "import java.util.Properties\n",
    "import java.io.File\n",
    "import org.apache.avro.generic.{GenericDatumReader, GenericRecord, GenericRecordBuilder, GenericData}\n",
    "import org.apache.avro.file.DataFileReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Kafka Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val props = new Properties()\n",
    "props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"broker:9092\")\n",
    "props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[KafkaAvroSerializer])\n",
    "props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[KafkaAvroSerializer])\n",
    "props.put(\"schema.registry.url\", \"http://schema_registry:8081\")\n",
    "val producer = new KafkaProducer[String, GenericRecord](props)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in pre-aggregated Avro data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val agg = new File(\"ipinyou_agg.avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val aggSchema = new org.apache.avro.Schema.Parser().parse(\"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"topLevelRecord\\\",\\\"fields\\\":[{\\\"name\\\":\\\"ad_exchange\\\",\\\"type\\\":\\\"int\\\"},{\\\"name\\\":\\\"ten_second_floor\\\",\\\"type\\\":[\\\"string\\\",\\\"null\\\"]},{\\\"name\\\":\\\"count\\\",\\\"type\\\":\\\"long\\\"}]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.text.SimpleDateFormat\n",
    "import java.util.Date\n",
    "import java.util.TimeZone\n",
    "\n",
    "def convertEpochTimestampToDate(time: Long, format: SimpleDateFormat): String = {\n",
    "    format.format(new Date(time))\n",
    "}\n",
    "\n",
    "def convertDateToEpochTimestamp(date: String, format: SimpleDateFormat): Long = {\n",
    "    format.parse(date).getTime\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce historical aggregate data to <b>ipinyou-agg</b> topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val datumReader = new GenericDatumReader[GenericRecord](aggSchema)\n",
    "val dataFileReader = new DataFileReader[GenericRecord](agg, datumReader)\n",
    "val currentTime = System.currentTimeMillis\n",
    "val sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n",
    "while (dataFileReader.hasNext) {\n",
    "    val maxTimestamp = 1382745560000L // latest timestamp found in avro data file\n",
    "    val record = dataFileReader.next\n",
    "    val recordTimestamp = convertDateToEpochTimestamp(record.get(\"ten_second_floor\").toString, sdf)\n",
    "    val newRecordTimestamp = convertEpochTimestampToDate(currentTime - (maxTimestamp - recordTimestamp), sdf)\n",
    "    record.put(1, newRecordTimestamp)\n",
    "    val producerRecord = new ProducerRecord[String, GenericRecord](\"ipinyou-agg\", record)\n",
    "    producer.send(producerRecord)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Now that we've backfilled our ipinyou-agg topic with some pre-aggregated data, we can set up our streaming aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jep.spark.avro.{SparkAvroConverter, SchemaRegistryService}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ss = spark\n",
    "import ss.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.catalyst.encoders.RowEncoder\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import com.databricks.spark.avro.SchemaConverters\n",
    "import org.apache.avro.{Schema, SchemaBuilder}\n",
    "import com.databricks.spark.avro._\n",
    "import org.apache.spark.sql.functions.{max, rank, window}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Schema Registry Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val schemaRegistry = new SchemaRegistryService(\"http://schema_registry:8081\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate StructType of desired fields from IPinYou avro schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@transient val schema = new org.apache.avro.Schema.Parser().parse(\"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"Ipinyou\\\",\\\"namespace\\\":\\\"com.conversantmedia.cake.avro\\\",\\\"doc\\\":\\\"Action\\\",\\\"fields\\\":[{\\\"name\\\":\\\"bid_id\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]},{\\\"name\\\":\\\"timestamp\\\",\\\"type\\\":[\\\"null\\\",\\\"long\\\"]},{\\\"name\\\":\\\"log_type\\\",\\\"type\\\":[\\\"null\\\",\\\"int\\\"]},{\\\"name\\\":\\\"ipinyou_id\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]},{\\\"name\\\":\\\"user_agent\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]},{\\\"name\\\":\\\"ip_address\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]},{\\\"name\\\":\\\"region\\\",\\\"type\\\":[\\\"null\\\",\\\"int\\\"]},{\\\"name\\\":\\\"city\\\",\\\"type\\\":[\\\"null\\\",\\\"int\\\"]},{\\\"name\\\":\\\"ad_exchange\\\",\\\"type\\\":[\\\"null\\\",\\\"int\\\"]},{\\\"name\\\":\\\"domain\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]},{\\\"name\\\":\\\"url\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]},{\\\"name\\\":\\\"anonymous_url_id\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]},{\\\"name\\\":\\\"ad_slot_id\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]},{\\\"name\\\":\\\"ad_slot_width\\\",\\\"type\\\":[\\\"null\\\",\\\"int\\\"]},{\\\"name\\\":\\\"ad_slot_height\\\",\\\"type\\\":[\\\"null\\\",\\\"int\\\"]},{\\\"name\\\":\\\"ad_slot_visibility\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]},{\\\"name\\\":\\\"ad_slot_format\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]},{\\\"name\\\":\\\"ad_slot_floor_price\\\",\\\"type\\\":[\\\"null\\\",\\\"int\\\"]},{\\\"name\\\":\\\"creative_id\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]},{\\\"name\\\":\\\"bidding_price\\\",\\\"type\\\":[\\\"null\\\",\\\"int\\\"]},{\\\"name\\\":\\\"paying_price\\\",\\\"type\\\":[\\\"null\\\",\\\"int\\\"]},{\\\"name\\\":\\\"landing_page_url\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]},{\\\"name\\\":\\\"advertiser_id\\\",\\\"type\\\":[\\\"null\\\",\\\"int\\\"]},{\\\"name\\\":\\\"user_tags\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]}],\\\"schemaId\\\":\\\"2\\\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.collection.JavaConversions._\n",
    "schema.getFields.map(_.name).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val fullStruct = SchemaConverters.toSqlType(schema).dataType.asInstanceOf[StructType]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildSubStruct(fields: Array[String], accStruct: StructType): StructType = {\n",
    "    if (fields.isEmpty) accStruct\n",
    "    else buildSubStruct(fields.tail, accStruct.add(fullStruct(fields.head)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val desiredFields = Array(\"ad_exchange\", \"log_type\", \"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val subStruct = buildSubStruct(desiredFields, new StructType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subStruct.printTreeString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Streaming Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ipinyou_stream = spark.readStream.\n",
    "    format(\"kafka\").\n",
    "    option(\"kafka.bootstrap.servers\", \"broker:9092\").\n",
    "    option(\"subscribe\", \"ipinyou\").\n",
    "    option(\"startingOffsets\", \"latest\").\n",
    "    load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipinyou_stream.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deserialize GenericRecords and convert to Spark SQL Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val converted = ipinyou_stream.select(\"value\").\n",
    "    as[Array[Byte]].\n",
    "    map(record => SparkAvroConverter.avroToRow(record, schemaRegistry, \"ipinyou\", subStruct))(RowEncoder(subStruct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalent Pseudo SQL:\n",
    "\n",
    "```SQL\n",
    "SELECT\n",
    "    CASE\n",
    "        WHEN ad_exchange IS NULL THEN -1\n",
    "        ELSE ad_exchange\n",
    "    END AS ad_exchange,\n",
    "    (floor(extract(second FROM to_timestamp(timestamp)) / 10) * 10)::STRING AS ten_second_floor,\n",
    "    count(1)\n",
    "FROM\n",
    "    ipinyou\n",
    "WHERE\n",
    "    log_type = 1\n",
    "GROUP BY\n",
    "    CASE\n",
    "        WHEN ad_exchange IS NULL THEN -1\n",
    "        ELSE ad_exchange\n",
    "    END AS ad_exchange,\n",
    "    (floor(extract(second FROM to_timestamp(timestamp)) / 10) * 10)::STRING\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val agg = converted.where(\"log_type = 1\").// impressions have log_type of 1\n",
    "    selectExpr(\"ad_exchange\", \"CAST(from_unixtime(timestamp/1000) as timestamp) as timestamp\").\n",
    "    na.\n",
    "    fill(-1, Seq(\"ad_exchange\")). // fill null ad_exchange values with -1\n",
    "    withWatermark(\"timestamp\", \"5 seconds\").\n",
    "    groupBy($\"ad_exchange\", window($\"timestamp\", \"10 seconds\")).\n",
    "    count.\n",
    "    selectExpr(\"ad_exchange\", \"cast(window.start as string) as ten_second_floor\", \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert streaming aggregate Rows back to serialized GenericRecords and write to new Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val bytes = agg.map(row => SparkAvroConverter.rowToAvro(row, schemaRegistry, \"ipinyou-agg\", \"Ipinyou.agg\", \"com.conversantmedia.cake.avro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val query = bytes.writeStream.\n",
    "    format(\"kafka\").\n",
    "    option(\"kafka.bootstrap.servers\", \"broker:9092\").\n",
    "    option(\"topic\", \"ipinyou-agg\").\n",
    "    trigger(Trigger.ProcessingTime(\"10 seconds\")).\n",
    "    option(\"checkpointLocation\", \"ipinyou_checkpoint\").\n",
    "    outputMode(\"append\").\n",
    "    start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "#### [Real-time visualization with Kibana](http://localhost:5601)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.2.1 - Scala",
   "language": "scala",
   "name": "spark_2.2.1_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
