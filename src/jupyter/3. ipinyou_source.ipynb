{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Producing IPinYou data to Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll be producing raw IPinYou data our <b>ipinyou</b> Kafka topic in order to similuate a live stream of data on which to perform our aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ShowTypes on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.avro.Schema\n",
    "import org.apache.avro.generic.GenericData\n",
    "import org.apache.avro.generic.GenericRecord\n",
    "import org.apache.kafka.clients.producer.KafkaProducer\n",
    "import org.apache.kafka.clients.producer.ProducerConfig\n",
    "import org.apache.kafka.clients.producer.ProducerRecord\n",
    "import io.confluent.kafka.serializers.{KafkaAvroDecoder, KafkaAvroSerializer}\n",
    "import java.util.Properties\n",
    "import java.io.File\n",
    "import org.apache.avro.generic.{GenericDatumReader, GenericRecord, GenericRecordBuilder, GenericData}\n",
    "import org.apache.avro.file.DataFileReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Kafka Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val props = new Properties()\n",
    "props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"broker:9092\")\n",
    "props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[KafkaAvroSerializer])\n",
    "props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[KafkaAvroSerializer])\n",
    "props.put(\"schema.registry.url\", \"http://schema_registry:8081\")\n",
    "val producer = new KafkaProducer[String, GenericRecord](props)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in IPinYou data from avro file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val file = new File(\"ipinyou_hour.avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rawSchema = new org.apache.avro.Schema.Parser().parse(\"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"Ipinyou\\\",\\\"namespace\\\":\\\"com.conversantmedia.cake.avro\\\",\\\"doc\\\":\\\"Action\\\",\\\"fields\\\":[{\\\"name\\\":\\\"bid_id\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]},{\\\"name\\\":\\\"timestamp\\\",\\\"type\\\":[\\\"null\\\",\\\"long\\\"]},{\\\"name\\\":\\\"log_type\\\",\\\"type\\\":[\\\"null\\\",\\\"int\\\"]},{\\\"name\\\":\\\"ipinyou_id\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]},{\\\"name\\\":\\\"user_agent\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]},{\\\"name\\\":\\\"ip_address\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]},{\\\"name\\\":\\\"region\\\",\\\"type\\\":[\\\"null\\\",\\\"int\\\"]},{\\\"name\\\":\\\"city\\\",\\\"type\\\":[\\\"null\\\",\\\"int\\\"]},{\\\"name\\\":\\\"ad_exchange\\\",\\\"type\\\":[\\\"null\\\",\\\"int\\\"]},{\\\"name\\\":\\\"domain\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]},{\\\"name\\\":\\\"url\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]},{\\\"name\\\":\\\"anonymous_url_id\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]},{\\\"name\\\":\\\"ad_slot_id\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]},{\\\"name\\\":\\\"ad_slot_width\\\",\\\"type\\\":[\\\"null\\\",\\\"int\\\"]},{\\\"name\\\":\\\"ad_slot_height\\\",\\\"type\\\":[\\\"null\\\",\\\"int\\\"]},{\\\"name\\\":\\\"ad_slot_visibility\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]},{\\\"name\\\":\\\"ad_slot_format\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]},{\\\"name\\\":\\\"ad_slot_floor_price\\\",\\\"type\\\":[\\\"null\\\",\\\"int\\\"]},{\\\"name\\\":\\\"creative_id\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]},{\\\"name\\\":\\\"bidding_price\\\",\\\"type\\\":[\\\"null\\\",\\\"int\\\"]},{\\\"name\\\":\\\"paying_price\\\",\\\"type\\\":[\\\"null\\\",\\\"int\\\"]},{\\\"name\\\":\\\"landing_page_url\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]},{\\\"name\\\":\\\"advertiser_id\\\",\\\"type\\\":[\\\"null\\\",\\\"int\\\"]},{\\\"name\\\":\\\"user_tags\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"]}],\\\"schemaId\\\":\\\"2\\\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val datumReader = new GenericDatumReader[GenericRecord](rawSchema)\n",
    "val dataFileReader = new DataFileReader[GenericRecord](file, datumReader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fast-forward each record's timestamp to simulate current events and produce to <b>ipinyou</b> Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val currentTime = System.currentTimeMillis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "while (dataFileReader.hasNext) {\n",
    "    val minTimestamp = 1382752800000L // earliest timestamp found in avro data file\n",
    "    val record = dataFileReader.next\n",
    "    val recordTimestamp = record.get(\"timestamp\").asInstanceOf[Long] * 1000\n",
    "    val newRecordTimestamp = (recordTimestamp - minTimestamp) + currentTime\n",
    "    record.put(1, newRecordTimestamp)\n",
    "    \n",
    "    val waitTime = newRecordTimestamp - System.currentTimeMillis\n",
    "    val producerRecord = new ProducerRecord[String, GenericRecord](\"ipinyou\", record)\n",
    "    if (waitTime > 0) Thread.sleep(waitTime)\n",
    "    \n",
    "    producer.send(producerRecord)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are producing a steady stream of data our <b>ipinyou</b> Kafka topic that will continue for the next hour.\n",
    "\n",
    "Notice that there is now an [**ipinyou** schema in Schema Registry.](http://localhost:8081/subjects/ipinyou-value/versions/latest)\n",
    "<hr>\n",
    "#### [4. Aggregating iPinYou Data](4. ipinyou_agg.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 2.2.1 - Scala",
   "language": "scala",
   "name": "spark_2.2.1_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
